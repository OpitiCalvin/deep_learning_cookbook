{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Book Recommendation Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from keras.utils import get_file\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "import xml.sax\n",
    "\n",
    "import subprocess\n",
    "import mwparserfromhell\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Wikipedia Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html>\n",
       " <head><title>Index of /enwiki/</title></head>\n",
       " <body bgcolor=\"white\">\n",
       " <h1>Index of /enwiki/</h1><hr/><pre><a href=\"../\">../</a>\n",
       " <a href=\"20180601/\">20180601/</a>                                          21-Jul-2018 01:33                   -\n",
       " <a href=\"20180620/\">20180620/</a>                                          02-Aug-2018 01:28                   -\n",
       " <a href=\"20180701/\">20180701/</a>                                          22-Aug-2018 01:25                   -\n",
       " <a href=\"20180720/\">20180720/</a>                                          02-Sep-2018 01:27                   -\n",
       " <a href=\"20180801/\">20180801/</a>                                          11-Aug-2018 08:29                   -\n",
       " <a href=\"20180820/\">20180820/</a>                                          23-Aug-2018 15:32                   -\n",
       " <a href=\"20180901/\">20180901/</a>                                          13-Sep-2018 00:25                   -\n",
       " <a href=\"latest/\">latest/</a>                                            13-Sep-2018 00:25                   -\n",
       " </pre><hr/></body>\n",
       " </html>, '\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = requests.get('https://dumps.wikimedia.org/enwiki/').text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "soup_index.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line of code finds the most recent dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180901/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the links that are dates of dumps\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href') and a.text[:-1].isdigit()]\n",
    "dumps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find the url extension for the actual XML data dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/enwiki/20180901/enwiki-20180901-pages-articles.xml.bz2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the html\n",
    "dump_html = requests.get('https://dumps.wikimedia.org/enwiki/' + dumps[-1]).text\n",
    "\n",
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "\n",
    "# Find the XML pages\n",
    "pages_xml = [a['href'] for a in soup_dump.find_all('a') if \n",
    "             a.has_attr('href') and a['href'].endswith('-pages-articles.xml.bz2')]\n",
    "pages_xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell finds the name of the file and the url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dumps.wikimedia.org//enwiki/20180901/enwiki-20180901-pages-articles.xml.bz2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_dump = pages_xml[0].rsplit('/')[-1]\n",
    "url = 'https://dumps.wikimedia.org/' + pages_xml[0]\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Data\n",
    "\n",
    "Now we need to actually download the data. This can be done using the keras `get_file` utility which downloads the specified file at the specified url. If we already have the entire dataset downloaded, then we don't want to download it again! For that reason we first use a check to see if the data exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded. File Size: 15.398410099 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "keras_home = '/data/wiki'\n",
    "data_path = keras_home + wikipedia_dump\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('Downloading')\n",
    "    data_path = get_file(wikipedia_dump, url)\n",
    "else:\n",
    "    print(f'Already downloaded. File Size: {os.stat(data_path).st_size / 1e9} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Through the Data\n",
    "\n",
    "Now we need to write a number of helper functions to extract the information we need from the data. A lot of these functions are copied directly from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Used to handle the XML wiki dump. Copied \n",
    "    directly from the book and only edited self._books (from self._movies)\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._books = []\n",
    "        self._curent_tag = None\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._curent_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name in ('title', 'text'):\n",
    "            self._curent_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._curent_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            book = process_article(**self._values)\n",
    "            if book:\n",
    "                self._books.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll get one example of a book to try and understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to  Process Article Text\n",
    "\n",
    "This function is also taken directly from the book with minor modifications. The biggest change is in the line `book = ` where the `infobox` has been changed to `infobox book` to reflect that we are searching for books!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_article(title, text, return_wikicode = False):\n",
    "    \"\"\"Process a wikipedia article looking for books\"\"\"\n",
    "    # Create a parsing object\n",
    "    wikicode = mwparserfromhell.parse(text)\n",
    "    if return_wikicode:\n",
    "        return wikicode\n",
    "    \n",
    "    # Search through templates for the book template\n",
    "    book = next((template for template in wikicode.filter_templates() \n",
    "                 if template.name.strip().lower() in ['infobox book']), None)\n",
    "    if book:\n",
    "        properties = {param.name.strip_code().strip(): param.value.strip_code().strip() \n",
    "                      for param in book.params\n",
    "                      if param.value.strip_code().strip()\n",
    "                     }\n",
    "        links = [x.title.strip_code().strip() for x in wikicode.filter_wikilinks()]\n",
    "        return (title, properties, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `bzcat` utility http://www.qnx.com/developers/docs/6.5.0SP1.update/com.qnx.doc.neutrino_utilities/b/bzcat.html which decompresses a bz2 compressed file and sends the contents to standard out. Effectively what this code is doing is decompressing the file one line at a time and sending the line through the `parser`. This gets around the need to load the entire file into memory at once since it is probably too large in its uncompressed state. \n",
    "\n",
    "The first time, we set the code to break if the handler encouters any books so we can look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                                         stdin = open(data_path), stdout = subprocess.PIPE).stdout):\n",
    "    x = line\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    if handler._books:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry is simply the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Animalia (book)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler._books[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second is all the parameters that are in the `infobox book` template on the wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '< !-- See Wikipedia:WikiProject_Novels or Wikipedia:WikiProject_Books -- >',\n",
       " 'name': 'Animalia',\n",
       " 'image': 'Animalia (book cover).jpg',\n",
       " 'alt': 'Book cover: a larger picture framed by smaller pictures, all of which contain different animals, and title with author at the top',\n",
       " 'author': 'Graeme Base',\n",
       " 'illustrator': 'Graeme Base',\n",
       " 'country': 'Australia',\n",
       " 'language': 'English',\n",
       " 'genre': 'Picture books',\n",
       " 'publisher': 'Harcourt Brace Jovanovich',\n",
       " 'release_date': '1986',\n",
       " 'pages': '32',\n",
       " 'isbn': '0-810-91868-4'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler._books[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry is all of the Wikipedia links that occur on the page. These are links that go to __other wikipedia pages__ as opposed to external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Graeme Base',\n",
       " 'Picture books',\n",
       " 'Harcourt Brace Jovanovich',\n",
       " \"Children's literature\",\n",
       " 'Graeme Base',\n",
       " 'alliteration',\n",
       " 'alphabet',\n",
       " 'alligator',\n",
       " 'butterfly',\n",
       " 'colouring book',\n",
       " 'Abrams Books',\n",
       " 'Animalia (TV series)',\n",
       " 'Venezuela',\n",
       " 'Minimax (TV channel)',\n",
       " 'Czech Republic',\n",
       " 'Slovakia',\n",
       " 'Greece',\n",
       " 'ET1 (Greece)',\n",
       " \"Australian Children's Television Foundation\",\n",
       " 'iPad',\n",
       " 'iPhone',\n",
       " 'iPod Touch',\n",
       " \"Children's Book Council of Australia\",\n",
       " \"Children's Book of the Year Award: Picture Book\",\n",
       " 'Category:Alphabet books',\n",
       " \"Category:1986 children's books\",\n",
       " 'Category:Picture books by Graeme Base',\n",
       " 'Category:Puzzle books',\n",
       " \"Category:Australian children's books\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler._books[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll gather this information for every article on Wikipedia that has an `infobox book` template on the page (this should be around 40,000). While this won't capture every book, it will give us a large selection to work with for making recommendations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{{Infobox book| < !-- See Wikipedia:WikiProject_Novels or Wikipedia:WikiProject_Books -- > \\n | name          = '''Animalia''' \\n | image         = Animalia (book cover).jpg \\n | caption       =  \\n | alt           = Book cover: a larger picture framed by smaller pictures, all of which contain different animals, and title with author at the top \\n | author        = [[Graeme Base]] \\n | illustrator   = Graeme Base \\n | country       = Australia \\n | language      = English  \\n | genre         = [[Picture books]] \\n | publisher     = [[Harcourt Brace Jovanovich]] \\n | release_date  = 1986 \\n | pages         = 32 \\n | isbn          = 0-810-91868-4 \\n | oclc          =  \\n }}\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = process_article(**handler._values, return_wikicode=True)\n",
    "processed.filter_templates()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Every Book on Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530 books found. 2787 seconds elapsed.\r"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# Object for handling xml\n",
    "handler = WikiXmlHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "start = timer()\n",
    "recorded_count = 0\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                                         stdin = open(data_path), stdout = subprocess.PIPE).stdout):\n",
    "    # Process the line (entry)\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "    # Print progress information\n",
    "    n_books = len(handler._books)\n",
    "    if (n_books % 10 == 0) and (n_books != recorded_count):\n",
    "        print(f'{n_books} books found. {round(timer() - start)} seconds elapsed.', end = '\\r')\n",
    "        # Make sure to only report found books once\n",
    "        recorded_count = n_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated/books.ndjson', 'wt') as fout:\n",
    "    for book in handler._books:\n",
    "         fout.write(json.dumps(book) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
